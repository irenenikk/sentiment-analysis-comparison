{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import binom\n",
    "from functools import reduce\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_list(tokens, min_freq=7):\n",
    "    \"\"\" Find bigrams from given text and return in a pandas dataframe. \"\"\"\n",
    "    bigrams = []\n",
    "    for i in range(1, len(tokens)):\n",
    "        bigram = f'{tokens[i-1]} {tokens[i]}'\n",
    "        bigrams += [bigram]\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uni_and_bi_grams(data_folder):\n",
    "    \"\"\" Find unigrams and bigrams in given text and return in a pandas dataframe. \"\"\"\n",
    "    sentiments = ['POS', 'NEG']    \n",
    "    # use lists to avoid calling append in a loop\n",
    "    # unigrams\n",
    "    unigrams = []\n",
    "    unigram_sentiments = []\n",
    "    unigram_file_ids = []\n",
    "    unigram_file_nos = []\n",
    "    # bigrams\n",
    "    bigrams = []\n",
    "    bigram_sentiments = []\n",
    "    bigram_file_ids = []\n",
    "    bigram_file_nos = []    \n",
    "    for sent in sentiments:\n",
    "        review_folder = f'{data_folder}/{sent}'\n",
    "        for file in os.listdir(review_folder):\n",
    "            # find unigrams\n",
    "            new_unigrams = pd.read_csv(os.path.join(review_folder, file), sep='\\t', header=None, names=['ngram', 'pos']).values[:,0]\n",
    "            unigrams += list(new_unigrams)\n",
    "            unigram_sentiments += [(1 if sent == 'POS' else -1)]*len(new_unigrams)\n",
    "            unigram_file_ids += [f'{sent}-{file[2:5]}']*len(new_unigrams)\n",
    "            unigram_file_nos += [int(file[2:5])]*len(new_unigrams)\n",
    "            # find bigrams\n",
    "            new_bigrams = get_bigram_list(new_unigrams)\n",
    "            bigrams += new_bigrams            \n",
    "            bigram_sentiments += [(1 if sent == 'POS' else -1)]*len(new_bigrams)\n",
    "            bigram_file_ids += [f'{sent}-{file[2:5]}']*len(new_bigrams)\n",
    "            bigram_file_nos += [int(file[2:5])]*len(new_bigrams)\n",
    "    unigram_df = pd.DataFrame(list(zip(unigrams, unigram_sentiments, unigram_file_ids, unigram_file_nos)), columns=['ngram', 'sentiment', 'file_id', 'file_no'])\n",
    "    bigram_df = pd.DataFrame(list(zip(bigrams, bigram_sentiments, bigram_file_ids, bigram_file_nos)), columns=['ngram', 'sentiment', 'file_id', 'file_no'])\n",
    "    return unigram_df, bigram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for defining the necessary probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_tokenize(content):\n",
    "    \"\"\" Split into unigrams by punctuation and whitespace, then lowercase and remove trailing whitespace\"\"\"\n",
    "    return np.asarray(list(filter(None,((map(lambda x: x, map(str.strip, re.split('(\\W)', content))))))))\n",
    "\n",
    "def bigram_tokenize(content):\n",
    "    \"\"\" Split text into bigrams \"\"\"\n",
    "    tokens = unigram_tokenize(content)\n",
    "    for i in range(1, len(tokens)):\n",
    "        yield f'{tokens[i-1]} {tokens[i]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_probabilities(ngrams, sent, min_count, smooth):\n",
    "    counts = ngrams[ngrams['sentiment'] == sent]['ngram'].value_counts()\n",
    "    filtered = counts[counts >= min_count]\n",
    "    voc_size = len(counts)\n",
    "    if smooth:\n",
    "        return (filtered+1)/(sum(filtered)+ngrams['ngram'].nunique())\n",
    "    return filtered/sum(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_probabilites(text, classes, tokenize, data, smooth, min_freq):\n",
    "    class_probs = np.zeros(len(classes))\n",
    "    for i, cl in enumerate(classes):\n",
    "        p = 0\n",
    "        conditioned_counts = get_ngram_probabilities(data, cl, min_freq, smooth)\n",
    "        smooth_denom = (data['sentiment']==cl).sum()+data['ngram'].nunique()\n",
    "        for word in tokenize(text):\n",
    "            if word in conditioned_counts.index:\n",
    "                p += np.log(conditioned_counts.loc[word])\n",
    "            # apply smoothing separately if word not present in class\n",
    "            elif smooth:\n",
    "                p += np.log(1/(smooth_denom))\n",
    "        # the prior is the fraction of documents in a specific class\n",
    "        sentiment_files = data[['file_id', 'sentiment']].groupby('file_id').mean()\n",
    "        prior = (sentiment_files['sentiment'] == cl).sum()/len(sentiment_files)\n",
    "        p += np.log(prior)\n",
    "        class_probs[i] = p\n",
    "    return class_probs\n",
    "\n",
    "def naive_binary_bayes(text, unigrams=None, bigrams=None, smooth=True):\n",
    "    \"\"\" Predict the class of a string given unigrams and bigrams. \"\"\"\n",
    "    if unigrams is None and bigrams is None:\n",
    "        raise ValueError('Please choose to use either unigrams or bigrams by providing the data')\n",
    "    # set the binary classification labels\n",
    "    classes = [-1, 1]\n",
    "    class_probs = np.zeros(len(classes))    \n",
    "    if unigrams is not None:\n",
    "        class_probs += get_class_probabilites(text, classes, unigram_tokenize, unigrams, smooth, min_freq=4)\n",
    "    if bigrams is not None:\n",
    "        class_probs += get_class_probabilites(text, classes, bigram_tokenize, bigrams, smooth, min_freq=7)\n",
    "    return classes[np.argmax(class_probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveB:\n",
    "        \n",
    "    def __init__(self, classes, unigrams=None, bigrams=None):\n",
    "        self.unigrams = unigrams\n",
    "        self.bigrams = bigrams\n",
    "        self.classes = classes\n",
    "        \n",
    "\n",
    "    def get_ngram_probabilities(ngrams, sent, min_count, smooth):\n",
    "        counts = ngrams[ngrams['sentiment'] == sent]['ngram'].value_counts()\n",
    "        filtered = counts[counts >= min_count]\n",
    "        voc_size = len(counts)\n",
    "        if smooth:\n",
    "            return (filtered+1)/(sum(filtered)+ngrams['ngram'].nunique())\n",
    "        return filtered/sum(filtered)    \n",
    "    \n",
    "    def get_class_probabilites(self, text, tokenize, data, smooth, min_freq):\n",
    "        class_probs = np.zeros(len(self.classes))\n",
    "        for i, cl in enumerate(self.classes):\n",
    "            p = 0\n",
    "            conditioned_counts = get_ngram_probabilities(data, cl, min_freq, smooth)\n",
    "            smooth_denom = (data['sentiment']==cl).sum()+data['ngram'].nunique()\n",
    "            for word in tokenize(text):\n",
    "                if word in conditioned_counts.index:\n",
    "                    p += np.log(conditioned_counts.loc[word])\n",
    "                # apply smoothing separately if word not present in class\n",
    "                elif smooth:\n",
    "                    p += np.log(1/(smooth_denom))\n",
    "            # the prior is the fraction of documents in a specific class\n",
    "            sentiment_files = data[['file_id', 'sentiment']].groupby('file_id').mean()\n",
    "            prior = (sentiment_files['sentiment'] == cl).sum()/len(sentiment_files)\n",
    "            p += np.log(prior)\n",
    "            class_probs[i] = p\n",
    "        return class_probs    \n",
    "        \n",
    "    def predict(self, text, training_data_files, smooth=True):\n",
    "        # set the binary classification labels\n",
    "        class_probs = np.zeros(len(self.classes))\n",
    "        if self.unigrams is not None:\n",
    "            training_unigrams = self.unigrams[self.unigrams['file_no'].isin(training_data_files)]\n",
    "            class_probs += self.get_class_probabilites(text, unigram_tokenize, training_unigrams, smooth, min_freq=4)\n",
    "        if self.bigrams is not None:\n",
    "            class_probs += self.get_class_probabilites(text, bigram_tokenize, self.bigrams[self.bigrams['file_no'].isin(training_data_files)], smooth, min_freq=7)\n",
    "        return self.classes[np.argmax(class_probs)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams, = get_uni_and_bi_grams('data-tagged')\n",
    "uni_naiveB = NaiveB([-1, 1], unigrams=unigrams)\n",
    "bi_naiveB = NaiveB([-1, 1], bigrams=bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_naiveB.predict('a great movie', list(range(899)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_naiveB.predict('this is a very bad movie', list(range(899)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_accuracy2(test_data, unigrams=None, bigrams=None):\n",
    "    \"\"\" Estimate the accuracy over test dataset using given unigrams and bigrams \"\"\"\n",
    "    acc = 0\n",
    "    for file_id, group in test_data.groupby('file_id'):\n",
    "        label = naive_binary_bayes(' '.join(group['ngram'].values), smooth=smooth, unigrams=unigrams, bigrams=bigrams)\n",
    "        # make sure each file is only associated with one sentiment\n",
    "        # otherwise there's a bug in reading of the data\n",
    "        assert(group['sentiment'].nunique() == 1)\n",
    "        acc += (label == group['sentiment'].unique()[0])\n",
    "    return acc/test_data['file_id'].nunique()\n",
    "\n",
    "def estimate_accuracy(test_data, training_data_files, naive_B, smooth):\n",
    "    \"\"\" Estimate the accuracy over test dataset using given unigrams and bigrams \"\"\"\n",
    "    acc = 0\n",
    "    i = 0\n",
    "    for file_id, group in test_data.groupby('file_id'):\n",
    "        i += 1\n",
    "        label = naive_B.predict(' '.join(group['ngram'].values), training_data_files, smooth=smooth)\n",
    "        # make sure each file is only associated with one sentiment\n",
    "        # otherwise there's a bug in reading of the data\n",
    "        assert(group['sentiment'].nunique() == 1)\n",
    "        print(acc/i)\n",
    "        acc += (label == group['sentiment'].unique()[0])\n",
    "    return acc/test_data['file_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_naiveB = NaiveB([-1, 1], unigrams=unigrams)\n",
    "bi_naiveB = NaiveB([-1, 1], bigrams=bigrams)\n",
    "uni_bi_naiveB = NaiveB([-1, 1], unigrams=unigrams, bigrams=bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.75\n",
      "0.8\n",
      "0.8333333333333334\n",
      "0.8571428571428571\n",
      "0.875\n",
      "0.8888888888888888\n",
      "0.9\n",
      "0.9090909090909091\n",
      "0.9166666666666666\n",
      "0.9230769230769231\n",
      "0.9285714285714286\n",
      "0.9333333333333333\n",
      "0.9375\n",
      "0.9411764705882353\n",
      "0.9444444444444444\n",
      "0.9473684210526315\n",
      "0.95\n",
      "0.9523809523809523\n",
      "0.9545454545454546\n",
      "0.9565217391304348\n",
      "0.9583333333333334\n",
      "0.96\n",
      "0.9615384615384616\n",
      "0.9259259259259259\n",
      "0.9285714285714286\n",
      "0.9310344827586207\n",
      "0.9333333333333333\n",
      "0.9354838709677419\n",
      "0.9375\n",
      "0.9393939393939394\n",
      "0.9411764705882353\n",
      "0.9428571428571428\n",
      "0.9444444444444444\n",
      "0.9459459459459459\n",
      "0.9473684210526315\n",
      "0.9487179487179487\n",
      "0.95\n",
      "0.9512195121951219\n",
      "0.9523809523809523\n",
      "0.9534883720930233\n",
      "0.9545454545454546\n",
      "0.9555555555555556\n",
      "0.9565217391304348\n",
      "0.9574468085106383\n",
      "0.9583333333333334\n",
      "0.9591836734693877\n",
      "0.96\n",
      "0.9607843137254902\n",
      "0.9615384615384616\n",
      "0.9622641509433962\n",
      "0.9629629629629629\n",
      "0.9636363636363636\n",
      "0.9642857142857143\n",
      "0.9649122807017544\n",
      "0.9655172413793104\n",
      "0.9661016949152542\n",
      "0.9666666666666667\n",
      "0.9672131147540983\n",
      "0.9516129032258065\n",
      "0.9523809523809523\n",
      "0.953125\n",
      "0.9538461538461539\n",
      "0.9545454545454546\n",
      "0.9552238805970149\n",
      "0.9558823529411765\n",
      "0.9565217391304348\n",
      "0.9571428571428572\n",
      "0.9577464788732394\n",
      "0.9583333333333334\n",
      "0.958904109589041\n",
      "0.9594594594594594\n",
      "0.96\n",
      "0.9605263157894737\n",
      "0.948051948051948\n",
      "0.9358974358974359\n",
      "0.9367088607594937\n",
      "0.9375\n",
      "0.9382716049382716\n",
      "0.9390243902439024\n",
      "0.9397590361445783\n",
      "0.9404761904761905\n",
      "0.9411764705882353\n",
      "0.9418604651162791\n",
      "0.9425287356321839\n",
      "0.9318181818181818\n",
      "0.9325842696629213\n",
      "0.9333333333333333\n",
      "0.9340659340659341\n",
      "0.9347826086956522\n",
      "0.9354838709677419\n",
      "0.9361702127659575\n",
      "0.9368421052631579\n",
      "0.9375\n",
      "0.9381443298969072\n",
      "0.9387755102040817\n",
      "0.9393939393939394\n",
      "0.94\n",
      "0.9405940594059405\n",
      "0.9411764705882353\n",
      "0.941747572815534\n",
      "0.9423076923076923\n",
      "0.9428571428571428\n",
      "0.9339622641509434\n",
      "0.9345794392523364\n",
      "0.9351851851851852\n",
      "0.9357798165137615\n",
      "0.9363636363636364\n",
      "0.9369369369369369\n",
      "0.9285714285714286\n",
      "0.9292035398230089\n",
      "0.9298245614035088\n",
      "0.9304347826086956\n",
      "0.9310344827586207\n",
      "0.9316239316239316\n",
      "0.9322033898305084\n",
      "0.9327731092436975\n",
      "0.9333333333333333\n",
      "0.9338842975206612\n",
      "0.9262295081967213\n",
      "0.926829268292683\n",
      "0.9274193548387096\n",
      "0.928\n",
      "0.9285714285714286\n",
      "0.9291338582677166\n",
      "0.9296875\n",
      "0.9302325581395349\n",
      "0.9307692307692308\n",
      "0.9312977099236641\n",
      "0.9318181818181818\n",
      "0.9323308270676691\n",
      "0.9328358208955224\n",
      "0.9333333333333333\n",
      "0.9338235294117647\n",
      "0.9343065693430657\n",
      "0.9347826086956522\n",
      "0.935251798561151\n",
      "0.9357142857142857\n",
      "0.9361702127659575\n",
      "0.9366197183098591\n",
      "0.9370629370629371\n",
      "0.9375\n",
      "0.9379310344827586\n",
      "0.9383561643835616\n",
      "0.9387755102040817\n",
      "0.9391891891891891\n",
      "0.9395973154362416\n",
      "0.94\n",
      "0.9403973509933775\n",
      "0.9407894736842105\n",
      "0.9411764705882353\n",
      "0.9415584415584416\n",
      "0.9419354838709677\n",
      "0.9423076923076923\n",
      "0.9426751592356688\n",
      "0.9430379746835443\n",
      "0.9433962264150944\n",
      "0.94375\n",
      "0.9440993788819876\n",
      "0.9382716049382716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5e80941811ae>\u001b[0m in \u001b[0;36mestimate_accuracy\u001b[0;34m(test_data, training_data_files, naive_B, smooth)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngram'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# make sure each file is only associated with one sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# otherwise there's a bug in reading of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-100595707533>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, training_data_files, smooth)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mclass_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtraining_unigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_no'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mclass_probs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class_probabilites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigram_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_unigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3604\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3605\u001b[0m         )\n\u001b[1;32m   3606\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1397\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m         )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1313\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m         )\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/naive-bayes/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m     )\n\u001b[0;32m-> 1721\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "single_split_train_files = list(range(899))\n",
    "test_data = unigrams[~unigrams['file_id'].isin(single_split_train_files)]\n",
    "estimate_accuracy(test_data, single_split_train_files, uni_naiveB, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_accuracy(test_data, single_split_train_files, bi_naiveB, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_accuracy(test_data, single_split_train_files, uni_bi_naiveB, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_accuracy(test_data, single_split_train_files, uni_naiveB, smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_value(N, k, q):\n",
    "    res = 0\n",
    "    for i in range(k+1):\n",
    "        res += binom.pmf(i, N, q)\n",
    "    return 2*res\n",
    "\n",
    "def sign_test(test_data, system_A, system_B, n=10):\n",
    "    plus, minus, null = 0, 0, 0\n",
    "    for i in range(len(test_data)):\n",
    "        inp = test_data['review'].iloc[i]\n",
    "        a = system_A(inp)\n",
    "        b = system_B(inp)\n",
    "        true_label = test_data['sentiment'].iloc[i]\n",
    "        if true_label == a:\n",
    "            plus += 1\n",
    "        elif true_label == b:\n",
    "            minus += 1\n",
    "        else:\n",
    "            null += 1\n",
    "    N = 2*math.ceil(null/2)+plus+minus\n",
    "    k = math.ceil(null/2)+min(plus, minus)\n",
    "    return calculate_p_value(N, k, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_unigram_bayes(data):\n",
    "    return uni_naiveB.predict(data, list(range(899)), smooth=True)\n",
    "\n",
    "def unsmoothed_unigram_bayes(data):\n",
    "    return uni_naiveB.predict(data, list(range(899)), smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_data(test_data):\n",
    "    # group by file_id to achieve full reviews\n",
    "    labelled_test_data = test_data.groupby('file_id')['ngram'].apply(lambda gs: list(gs)).reset_index()\n",
    "    labelled_test_data['review'] = labelled_test_data['ngram'].apply(lambda x: ' '.join(x[0]))\n",
    "    labelled_test_data = labelled_test_data.drop('ngram', axis=1)\n",
    "    labelled_test_data['sentiment'] = labelled_test_data['file_id'].apply(lambda x: 1 if x[:3] == 'POS' else -1)\n",
    "    return labelled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_test_data = build_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "p_value = sign_test(labelled_test_data, smoothed_unigram_bayes, unsmoothed_unigram_bayes)\n",
    "p_value\n",
    "# 6.269514409998372e-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variance(data):\n",
    "    mean = np.mean(data)\n",
    "    return np.sum(np.square(data-mean))\n",
    "\n",
    "def cross_validate(naive_B, data, folds):\n",
    "    file_amount = data['file_no'].nunique()\n",
    "    indx = np.arange(0, file_amount, folds)\n",
    "    scores = np.zeros(folds)\n",
    "    for f in range(folds):\n",
    "        test_data_mask = data['file_no'].isin(indx+f)\n",
    "        training_file_ids = data[~test_data_mask]['file_no'].unique()\n",
    "        test_data = data[test_data_mask]\n",
    "        acc = estimate_accuracy(test_data, training_file_ids, naive_B, smooth=True)\n",
    "        print(acc)\n",
    "        scores[f] = acc\n",
    "    return np.mean(scores), sample(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "acc_mean, acc_var = cross_validate(uni_naiveB, unigrams, 10)\n",
    "# accuracies\n",
    "# results\n",
    "# mean: 81.10\n",
    "# variance: 0.0070224999999999845"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset:\n",
    "When using this dataset, please cite our ACL 2011 paper:\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_folder = 'aclImdb'\n",
    "imdb_sentiments = ['pos', 'neg']\n",
    "subfolders = ['train', 'test']\n",
    "review_list = []\n",
    "review_id_list = []\n",
    "review_grade_list = []\n",
    "for sent in imdb_sentiments:\n",
    "    for subf in subfolders:\n",
    "        for review_file in os.listdir(os.path.join(imdb_data_folder, subf, sent)):\n",
    "            idd = review_file.split('_')[0]\n",
    "            review_id_list += [idd]\n",
    "            grade = re.search('_(.*)\\.txt', review_file).group(1)\n",
    "            review_grade_list += [grade]\n",
    "            f = open(os.path.join(imdb_data_folder, subf, sent, review_file), 'r+')\n",
    "            review = f.read()\n",
    "            review_list += [review]\n",
    "reviews = pd.DataFrame(list(zip(review_list, review_id_list, review_grade_list)), columns=['review', 'id', 'grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_tokenize(doc):\n",
    "    return [x.lower() for x in re.sub(r'[^a-zA-Z\\s]', '', a).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc_tokenize(doc), [i]) for i, doc in enumerate(reviews['review'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 100\n",
    "window_size = 2\n",
    "min_count = 4\n",
    "model = Doc2Vec(documents, vector_size=vec_size, window=2, min_count=min_count, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(f'doc2vec_{vec_size}_{window_size}_{min_count}')\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:naive-bayes] *",
   "language": "python",
   "name": "conda-env-naive-bayes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
